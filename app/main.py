import streamlit as st
import os
import re
import time
from urllib.parse import urljoin
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader

# --- æ–°è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
import trafilatura
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from duckduckgo_search import DDGS

# --- è¨­å®š ---
OLLAMA_URL = st.secrets.get("OLLAMA_BASE_URL", "http://localhost:11435")
MODEL_NAME = st.secrets.get("MODEL_NAME", "nemotron-3-nano")

st.set_page_config(
    page_title="è¦ç´„ãã‚“ Deep Research",
    page_icon="ğŸ•µï¸â€â™‚ï¸",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- UI/UX: ãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š ---
st.markdown("""
<style>
    /* å…¨ä½“ãƒ†ãƒ¼ãƒ */
    .stApp { background-color: #121212 !important; color: #e0e0e0 !important; font-family: 'Hiragino Kaku Gothic ProN', sans-serif !important; }
    
    /* å…¥åŠ›æ¬„ */
    .stTextArea > div > div > textarea {
        background-color: #1e1e1e !important; color: white !important; border: 1px solid #444 !important; border-radius: 12px;
    }
    
    /* ãƒœã‚¿ãƒ³ */
    .stButton > button {
        background: linear-gradient(90deg, #d946ef, #8b5cf6) !important; /* Agentã£ã½ã„ç´«ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ */
        color: white !important; border: none !important; font-weight: bold !important; padding: 16px; border-radius: 12px;
    }
    
    /* ãƒ†ãƒ¼ãƒ–ãƒ« */
    [data-testid="stMarkdownContainer"] table { display: block; overflow-x: auto; white-space: nowrap; border-collapse: collapse; border: 1px solid #333; margin: 20px 0; }
    [data-testid="stMarkdownContainer"] th { background-color: #2d2d2d !important; color: #fff; padding: 10px; border-bottom: 2px solid #555; }
    [data-testid="stMarkdownContainer"] td { padding: 10px; border-bottom: 1px solid #333; background-color: #1e1e1e; }

    /* ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º */
    .stStatusWidget { background-color: #1e1e1e !important; border: 1px solid #333 !important; }
</style>
""", unsafe_allow_html=True)

# --- ãƒ„ãƒ¼ãƒ«é–¢æ•°: æ¤œç´¢ã¨å–å¾— ---

@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def search_web(query, max_results=3):
    """DuckDuckGoã§Webæ¤œç´¢ã‚’è¡Œã†"""
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='jp-jp', safesearch='off', max_results=max_results))
        return results # [{'title':..., 'href':..., 'body':...}, ...]
    except Exception as e:
        print(f"Search Error: {e}")
        return []

def clean_text(text):
    if not text: return ""
    text = text.replace('\x00', '')
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

@retry(stop=stop_after_attempt(2), wait=wait_fixed(2), retry=retry_if_exception_type(Exception))
def fetch_content(url):
    """URLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º (Trafilatura)"""
    try:
        if url.lower().endswith('.pdf'):
            return "PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆç¾åœ¨ã¯Webãƒšãƒ¼ã‚¸ã®ã¿å¯¾å¿œï¼‰"
        
        downloaded = trafilatura.downloads.fetch_url(url)
        if downloaded is None: return ""
        
        text = trafilatura.extract(downloaded, include_comments=False, include_tables=True)
        return clean_text(text) if text else ""
    except Exception:
        return ""

# --- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€è€ƒãƒ­ã‚¸ãƒƒã‚¯ ---

def get_llm():
    return ChatOllama(
        model=MODEL_NAME,
        base_url=OLLAMA_URL,
        temperature=0.3,
        headers={"ngrok-skip-browser-warning": "true"},
        keep_alive="5m"
    )

def plan_research(topic):
    """ã€è¨ˆç”»ã€‘ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒˆãƒ”ãƒƒã‚¯ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template("""
    ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼ï¼šã€Œ{topic}ã€
    
    ã“ã®ä¾é ¼ã‚’é”æˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’é›†ã‚ã‚‹ãŸã‚ã®ã€ŒWebæ¤œç´¢ã‚¯ã‚¨ãƒªã€ã‚’3ã¤è€ƒãˆã¦ãã ã•ã„ã€‚
    
    å‡ºåŠ›å½¢å¼:
    - ã‚¯ã‚¨ãƒª1
    - ã‚¯ã‚¨ãƒª2
    - ã‚¯ã‚¨ãƒª3
    (ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã€‚ã‚¯ã‚¨ãƒªã®ã¿ã‚’ç®‡æ¡æ›¸ãã§å‡ºåŠ›)
    """)
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"topic": topic})
    queries = [line.strip("- ").strip() for line in response.split("\n") if line.strip()]
    return queries[:3] # æœ€å¤§3ã¤

def analyze_findings(topic, current_notes):
    """ã€ä¿®æ­£ã€‘é›†ã¾ã£ãŸæƒ…å ±ã‚’åˆ†æã—ã€ä¸è¶³æƒ…å ±ã‚’ç‰¹å®šã™ã‚‹"""
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template("""
    ç¾åœ¨ã®èª¿æŸ»ãƒ†ãƒ¼ãƒï¼šã€Œ{topic}ã€
    ã“ã‚Œã¾ã§ã®èª¿æŸ»ãƒãƒ¼ãƒˆï¼š
    {notes}
    
    ä¸Šè¨˜ã®æƒ…å ±ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼ã«ç­”ãˆã‚‹ã®ã«ååˆ†ã§ã™ã‹ï¼Ÿ
    ã‚‚ã—ä¸è¶³ãŒã‚ã‚Œã°ã€è¿½åŠ ã§ä½•ã‚’æ¤œç´¢ã™ã¹ãã‹ã€å…·ä½“çš„ãªã€Œè¿½åŠ æ¤œç´¢ã‚¯ã‚¨ãƒªã€ã‚’1ã¤ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    ååˆ†ã§ã‚ã‚Œã° "SUFFICIENT" ã¨ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """)
    chain = prompt | llm | StrOutputParser()
    response = chain.invoke({"topic": topic, "notes": current_notes[:10000]})
    return response.strip()

def summarize_page(topic, url, content):
    """ã€èª­è§£ã€‘Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ãƒ¡ãƒ¢åŒ–ã™ã‚‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç¯€ç´„ï¼‰"""
    if len(content) < 200: return "" # å†…å®¹ãŒè–„ã™ãã‚‹å ´åˆã¯ç„¡è¦–
    
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template("""
    ãƒ†ãƒ¼ãƒï¼šã€Œ{topic}ã€
    
    ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‹ã‚‰ã€ãƒ†ãƒ¼ãƒã«é–¢é€£ã™ã‚‹é‡è¦ãªäº‹å®Ÿã€æ•°å€¤ã€æ„è¦‹ã‚’æŠ½å‡ºã—ã¦ã€æ—¥æœ¬èªã®çŸ­ã„ãƒ¡ãƒ¢ã«ã—ã¦ãã ã•ã„ã€‚
    ç„¡é–¢ä¿‚ãªéƒ¨åˆ†ã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚
    
    Webãƒšãƒ¼ã‚¸å†…å®¹:
    {content}
    """)
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº¢ã‚Œé˜²æ­¢ã®ãŸã‚ãƒšãƒ¼ã‚¸å†…å®¹ã¯åˆ‡ã‚Šè©°ã‚ã‚‹
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"topic": topic, "content": content[:8000]})

def write_final_report(topic, all_notes):
    """ã€çµ±åˆã€‘æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template("""
    ã‚ãªãŸã¯æœ€é«˜å³°ã®ãƒ¬ãƒãƒ¼ãƒˆä½œæˆAIã§ã™ã€‚
    ä»¥ä¸‹ã®ã€Œèª¿æŸ»ãƒãƒ¼ãƒˆã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ãƒ¼ãƒã€Œ{topic}ã€ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

    ã€èª¿æŸ»ãƒãƒ¼ãƒˆã€‘
    {notes}

    ã€å‡ºåŠ›å½¢å¼ã€‘
    # {topic} ã«é–¢ã™ã‚‹èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ
    
    ## ğŸ¯ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
    ï¼ˆçµè«–ã‚’ç°¡æ½”ã«ï¼‰
    
    ## ğŸ” èª¿æŸ»çµæœè©³ç´°
    ï¼ˆè¦‹å‡ºã—ã‚’åˆ†ã‘ã¦æ§‹é€ çš„ã«è¨˜è¿°ã€‚æ•°å€¤ã‚„æ¯”è¼ƒã¯Markdownã®è¡¨ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ï¼‰
    
    ## ğŸ’¡ è€ƒå¯Ÿãƒ»ç¤ºå”†
    ï¼ˆé›†ã‚ã‚‰ã‚ŒãŸæƒ…å ±ã‹ã‚‰è¨€ãˆã‚‹ã“ã¨ï¼‰
    
    â€»å¿…ãšæ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """)
    return prompt | llm | StrOutputParser()

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---

def run_deep_research(topic, status_container):
    all_notes = ""
    visited_urls = set()
    
    # 1. è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º
    status_container.write("ğŸ¤” èª¿æŸ»è¨ˆç”»ã‚’ç«‹æ¡ˆä¸­...")
    queries = plan_research(topic)
    status_container.write(f"ğŸ“‹ æ¤œç´¢ãƒ—ãƒ©ãƒ³: {queries}")
    
    # 2. å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º (ãƒ©ã‚¦ãƒ³ãƒ‰1)
    status_container.write("ğŸŒ Webèª¿æŸ»ã‚’é–‹å§‹ (Round 1)...")
    for q in queries:
        status_container.write(f"ğŸ” æ¤œç´¢ä¸­: {q}...")
        results = search_web(q, max_results=2)
        
        for res in results:
            url = res['href']
            if url in visited_urls: continue
            visited_urls.add(url)
            
            status_container.write(f"ğŸ“– èª­è§£ä¸­: {res['title']}...")
            content = fetch_content(url)
            if content:
                summary = summarize_page(topic, url, content)
                all_notes += f"\n--- Source: {res['title']} ({url}) ---\n{summary}\n"
    
    # 3. ä¿®æ­£ãƒ•ã‚§ãƒ¼ã‚º (è‡ªå¾‹åˆ¤æ–­)
    status_container.write("ğŸ§  æƒ…å ±ã®å……è¶³åº¦ã‚’ç¢ºèªä¸­...")
    gap_analysis = analyze_findings(topic, all_notes)
    
    if "SUFFICIENT" not in gap_analysis and len(gap_analysis) < 50: # çŸ­ã„ã‚¯ã‚¨ãƒªãŒè¿”ã£ã¦ããŸå ´åˆ
        new_query = gap_analysis.replace('"', '').strip()
        status_container.write(f"ğŸš€ è¿½åŠ èª¿æŸ»ãŒå¿…è¦ã¨åˆ¤æ–­: ã€Œ{new_query}ã€ã‚’èª¿æŸ»ã—ã¾ã™")
        
        results = search_web(new_query, max_results=2)
        for res in results:
            url = res['href']
            if url in visited_urls: continue
            
            status_container.write(f"ğŸ“– è¿½åŠ èª­è§£ä¸­: {res['title']}...")
            content = fetch_content(url)
            if content:
                summary = summarize_page(topic, url, content)
                all_notes += f"\n--- Source: {res['title']} ({url}) ---\n{summary}\n"
    else:
        status_container.write("âœ… ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã‚Šã¾ã—ãŸã€‚")

    # 4. çµ±åˆãƒ•ã‚§ãƒ¼ã‚º
    status_container.write("âœï¸ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...")
    return write_final_report(topic, all_notes), all_notes

# --- UIæ§‹ç¯‰ ---

st.title("è¦ç´„ãã‚“ Deep Research")
st.caption("è‡ªå¾‹å‹AIãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

st.markdown("""
<div style="background-color: #262626; padding: 15px; border-radius: 10px; border-left: 5px solid #d946ef; margin-bottom: 20px;">
    <strong>ğŸ’¡ ä½¿ã„æ–¹:</strong> URLã§ã¯ãªãã€ã€ŒçŸ¥ã‚ŠãŸã„ã“ã¨ã€ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚<br>
    ä¾‹ï¼šã€Œæœ€æ–°ã®é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®æŠ€è¡“å‹•å‘ã¨ã€ä¸»è¦ä¼æ¥­ã®ã‚·ã‚§ã‚¢ã«ã¤ã„ã¦èª¿ã¹ã¦ã€
</div>
""", unsafe_allow_html=True)

topic_input = st.text_area("ãƒªã‚µãƒ¼ãƒãƒ†ãƒ¼ãƒã‚’å…¥åŠ›", height=100, placeholder="ã“ã“ã«èª¿æŸ»ã—ãŸã„ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

if st.button("ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ (Start Agent)"):
    if not topic_input:
        st.warning("ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.status("ğŸš€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹•...", expanded=True) as status:
            try:
                # ãƒªã‚µãƒ¼ãƒå®Ÿè¡Œ
                report_chain, raw_notes = run_deep_research(topic_input, status)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ç”¨ã‚³ãƒ³ãƒ†ãƒŠ
                st.markdown("---")
                output_container = st.empty()
                full_response = ""
                
                # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨è¡¨ç¤º
                for chunk in report_chain.stream({"topic": topic_input, "notes": raw_notes}):
                    full_response += chunk
                    output_container.markdown(full_response)
                
                status.update(label="ãƒªã‚µãƒ¼ãƒå®Œäº†ï¼", state="complete", expanded=False)
                
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªç”¨
                with st.expander("ğŸ“š åé›†ã•ã‚ŒãŸèª¿æŸ»ãƒãƒ¼ãƒˆ (Raw Data)"):
                    st.text(raw_notes)
                    
            except Exception as e:
                status.update(label="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", state="error")
                st.error(f"Agent Error: {e}")
